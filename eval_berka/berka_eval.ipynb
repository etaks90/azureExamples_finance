{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://webpages.charlotte.edu/mirsad/itcs6265/group1/domain.html#:~:text=The%20Berka%20dataset%20is%20a,clients%20with%20approximately%201%2C000%2C000%20transactions.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "#############################################################################################################################\n",
    "################################################  READ FILES    #############################################################\n",
    "#############################################################################################################################\n",
    "\n",
    "# account\n",
    "df_account = pd.read_csv(r\"C:\\Users\\oliver.koehn\\Documents\\gitProjects\\azureExamples_finance\\eval_berka\\in\\account.asc\", delimiter=\";\")\n",
    "df_account[\"acc_dist_id\"] = df_account[\"district_id\"]\n",
    "df_account['date'] = pd.to_datetime(\"19\" + df_account['date'].astype(str), format='%Y-%m-%d')\n",
    "df_account = df_account.rename(columns={'date': 'date_acc'})\n",
    "\n",
    "# card\n",
    "df_card = pd.read_csv(r\"C:\\Users\\oliver.koehn\\Documents\\gitProjects\\azureExamples_finance\\eval_berka\\in\\card.asc\", delimiter=\";\")\n",
    "df_card['issued'] = pd.to_datetime(\"19\" + df_card['issued'].astype(str), format='%Y-%m-%d')\n",
    "df_card = df_card.rename(columns={'type': 'card_type'})\n",
    "\n",
    "# client\n",
    "df_client = pd.read_csv(r\"C:\\Users\\oliver.koehn\\Documents\\gitProjects\\azureExamples_finance\\eval_berka\\in\\client.asc\", delimiter=\";\")\n",
    "df_client[\"cli_dist_id\"] = df_client[\"district_id\"]\n",
    "df_client[\"MM\"] = df_client['birth_number']//100 - df_client['birth_number']//10000*100\n",
    "df_client['gender'] = 'M'\n",
    "# it sa strang eformat where gende ris encoded like this\n",
    "df_client.loc[df_client['MM']>50,'gender'] = 'F'\n",
    "df_client.loc[df_client['gender']=='F','birth_number'] -= 5000\n",
    "df_client['birth_number'] = pd.to_datetime(\"19\" + df_client['birth_number'].astype(str), format='%Y-%m-%d')\n",
    "df_client.rename(columns = {'birth_number':'date_birth'}, inplace=True)\n",
    "df_client.drop('MM',1,inplace=True)\n",
    "\n",
    "# disp\n",
    "df_disp = pd.read_csv(r\"C:\\Users\\oliver.koehn\\Documents\\gitProjects\\azureExamples_finance\\eval_berka\\in\\disp.asc\", delimiter=\";\")\n",
    "df_disp = df_disp.rename(columns={'type': 'disp_type'})\n",
    "\n",
    "# district\n",
    "df_district = pd.read_csv(r\"C:\\Users\\oliver.koehn\\Documents\\gitProjects\\azureExamples_finance\\eval_berka\\in\\district.asc\", delimiter=\";\")\n",
    "df_district = df_district.rename(columns={'A1': 'district_id'})\n",
    "\n",
    "# loan\n",
    "df_loan = pd.read_csv(r\"C:\\Users\\oliver.koehn\\Documents\\gitProjects\\azureExamples_finance\\eval_berka\\in\\loan.asc\", delimiter=\";\")\n",
    "df_loan['date'] = pd.to_datetime(\"19\" + df_loan['date'].astype(str), format='%Y-%m-%d')\n",
    "df_loan = df_loan.rename(columns={'date': 'date_loan'})\n",
    "\n",
    "# order\n",
    "df_order = pd.read_csv(r\"C:\\Users\\oliver.koehn\\Documents\\gitProjects\\azureExamples_finance\\eval_berka\\in\\order.asc\", delimiter=\";\")\n",
    "df_order = df_order.rename(columns={'amount': 'order_amount'})\n",
    "df_order['order_amount'] = df_order.order_amount.astype('float')\n",
    "df_order_average = df_order.copy()\n",
    "df_order_average = df_order_average.groupby('account_id').mean()\n",
    "df_order_average = df_order_average.rename(columns={'order_amount': 'average_order_amount'})\n",
    "df_order_average = df_order_average[[\"order_id\", \"average_order_amount\"]]\n",
    "\n",
    "# trans\n",
    "df_trans = pd.read_csv(r\"C:\\Users\\oliver.koehn\\Documents\\gitProjects\\azureExamples_finance\\eval_berka\\in\\trans.asc\", delimiter=\";\")\n",
    "df_trans['date'] = pd.to_datetime(\"19\" + df_trans['date'].astype(str), format='%Y-%m-%d')\n",
    "df_trans_average = df_trans.copy()\n",
    "df_trans_average = df_trans_average.groupby('account_id').mean()\n",
    "df_trans_average[\"account_id\"] = df_trans_average.index\n",
    "df_trans_average = df_trans_average.rename(columns={'amount': 'average_trans_amount', 'balance': 'average_trans_balance'})\n",
    "df_trans_average = df_trans_average[['average_trans_amount', 'average_trans_balance']]\n",
    "df_n_trans = df_trans.groupby('account_id').count()\n",
    "df_n_trans = df_n_trans.rename(columns={'trans_id': 'n_trans'})\n",
    "df_n_trans = df_n_trans[[\"n_trans\"]]\n",
    "\n",
    "#############################################################################################################################\n",
    "################################################      JOIN      #############################################################\n",
    "#############################################################################################################################\n",
    "\n",
    "\n",
    "df = df_loan.merge(df_account, on = \"account_id\", how = \"inner\")\\\n",
    "    .merge(df_district, on = \"district_id\", how = \"inner\")\\\n",
    "    .merge(df_order_average, on = \"account_id\", how = \"inner\")\\\n",
    "    .merge(df_trans_average, on = \"account_id\", how = \"inner\")\\\n",
    "    .merge(df_disp, on = \"account_id\", how = \"inner\")\\\n",
    "    .merge(df_card, on = \"disp_id\", how = \"left\")\\\n",
    "    .merge(df_client, on = \"client_id\", how = \"inner\")\\\n",
    "    .merge(df_n_trans, on = \"account_id\", how = \"inner\")\\\n",
    "\n",
    "df = df[df[\"disp_type\"] == \"OWNER\"]\n",
    "\n",
    "# additional fields\n",
    "df['days_between'] = (df['date_loan'] - df['date_acc']).dt.days\n",
    "df['n_inhabitants'] = df.A4\n",
    "df['average_salary'] = df.A11\n",
    "df['average_unemployment_rate'] = df[['A12', 'A13']].mean(axis=1)\n",
    "df['entrepreneur_rate'] = df['A14']\n",
    "df['average_crime_rate'] = df[['A15', 'A16']].mean(axis=1) / df['n_inhabitants']\n",
    "df['default'] = (df['status'] == 'B') | (df['status'] == 'D')\n",
    "\n",
    "df['same_district'] = df['acc_dist_id'] == df['cli_dist_id']\n",
    "df['date_opening_loan'] = pd.to_datetime(df['date_loan'], format='%Y-%m-%d')\n",
    "df['owner_age_at_opening'] = (df['date_opening_loan']  - df['date_birth']).dt.days // 365\n",
    "\n",
    "cat_cols = [\"frequency\", \"card_type\", \"gender\"]\n",
    "num_cols = ['amount', 'duration', 'payments', 'days_between', 'n_inhabitants', \n",
    "            'average_salary', 'average_unemployment_rate', 'entrepreneur_rate', \n",
    "            'average_crime_rate', 'average_order_amount', 'average_trans_amount',\n",
    "            'average_trans_balance', 'n_trans', 'owner_age_at_opening', \n",
    "            'same_district']\n",
    "list_features = num_cols + cat_cols\n",
    "\n",
    "df[list_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "sns.heatmap(\n",
    "        df[list_features + [\"default\"]].corr(), \n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap = True),\n",
    "        square=True, \n",
    "        cbar=False,\n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
    "        annot_kws={'fontsize':12 })\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# https://scikit-learn.org/1.0/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions\n",
    "\n",
    "col_trans = ColumnTransformer([\n",
    "    ('num', MinMaxScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(drop='if_binary'), cat_cols)\n",
    "])\n",
    "\n",
    "df_transformed = col_trans.fit_transform(df[list_features])\n",
    "X = df_transformed[:, :]\n",
    "y = df[\"default\"].map({False: 0, True: 1})\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=10)\n",
    "\n",
    "# oversampling is important, check difference!!\n",
    "# Apply oversampling to the training set\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# See the inital model performance\n",
    "clf = RandomForestClassifier(random_state=10)\n",
    "print('Acc:', cross_val_score(clf, X_train, y_train, cv=StratifiedKFold(n_splits=5), scoring='accuracy').mean())\n",
    "print('F1:', cross_val_score(clf, X_train, y_train, cv=StratifiedKFold(n_splits=5), scoring='f1').mean())\n",
    "print('ROC AUC:', cross_val_score(clf, X_train, y_train, cv=StratifiedKFold(n_splits=5), scoring='roc_auc').mean())\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes long\n",
    "if 1 == 2:\n",
    "    clf = GridSearchCV(RandomForestClassifier(random_state=10), param_grid=params, \n",
    "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=10), scoring='f1')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(clf.best_params_) # {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 10}\n",
    "    print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train network\n",
    "clf = RandomForestClassifier(n_estimators=10,\n",
    "                             max_depth=10,\n",
    "                             min_samples_split=5,\n",
    "                             min_samples_leaf=2,\n",
    "                             random_state=11)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "try:\n",
    "    df = df.reset_index()\n",
    "except:\n",
    "    pass\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([('transformer', col_trans),('classifier', clf)])\n",
    "j_berka = {}\n",
    "j_berka[\"network\"] = pipeline\n",
    "j_berka[\"df\"] = df\n",
    "j_berka[\"num_cols\"] = num_cols\n",
    "j_berka[\"cat_cols\"] = cat_cols\n",
    "\n",
    "with open('berka.pickle', 'wb') as f:\n",
    "    pickle.dump(j_berka, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yields same result as below\n",
    "\n",
    "j = []\n",
    "for i in range(len(df)):\n",
    "    j_dummy = {}\n",
    "    j_dummy[\"true\"] = 1 if df[\"default\"][i] else 0\n",
    "    x = df[num_cols + cat_cols].iloc[i].to_frame().T\n",
    "    j_dummy[\"pred\"] = pipeline.predict(x)[0]\n",
    "    j.append(j_dummy)\n",
    "    \n",
    "df_pred = pd.DataFrame(j)\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for i, r in df_pred.iterrows():\n",
    "    if (r[\"true\"] == 1) and (r[\"pred\"] == 1):\n",
    "        tp = tp + 1\n",
    "    elif (r[\"true\"] == 1) and (r[\"pred\"] == 0):\n",
    "        fn = fn + 1\n",
    "    elif (r[\"true\"] == 0) and (r[\"pred\"] == 1):\n",
    "        fp = fp + 1\n",
    "    elif (r[\"true\"] == 0) and (r[\"pred\"] == 0):\n",
    "        tn = tn + 1\n",
    "\n",
    "print(\"tp: \" + str(tp))\n",
    "print(\"tn: \" + str(tn))\n",
    "print(\"fp: \" + str(fp))\n",
    "print(\"fn: \" + str(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_train_proba = clf.predict_proba(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_test_proba = clf.predict_proba(X_test)\n",
    "\n",
    "print('Train Acc:', accuracy_score(y_train, y_train_pred))\n",
    "print('Train F1:', f1_score(y_train, y_train_pred))\n",
    "print('Train ROC AUC:', roc_auc_score(y_train, y_train_proba[:, 1]))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)\n",
    "\n",
    "print('test Acc:', accuracy_score(y_test, y_test_pred))\n",
    "print('test F1:', f1_score(y_test, y_test_pred))\n",
    "print('test ROC AUC:', roc_auc_score(y_test, y_test_proba[:, 1]))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('berka.pickle', 'rb') as f:\n",
    "    berka = pickle.load(f)\n",
    "\n",
    "df = berka[\"df\"]\n",
    "pipeline = berka[\"network\"]\n",
    "\n",
    "# yields same result as below\n",
    "\n",
    "j = []\n",
    "for i in range(len(df)):\n",
    "    j_dummy = {}\n",
    "    j_dummy[\"true\"] = 1 if df[\"default\"][i] else 0\n",
    "    x = df[num_cols + cat_cols].iloc[i].to_frame().T\n",
    "    j_dummy[\"pred\"] = pipeline.predict(x)[0]\n",
    "    j.append(j_dummy)\n",
    "    \n",
    "df_pred = pd.DataFrame(j)\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for i, r in df_pred.iterrows():\n",
    "    if (r[\"true\"] == 1) and (r[\"pred\"] == 1):\n",
    "        tp = tp + 1\n",
    "    elif (r[\"true\"] == 1) and (r[\"pred\"] == 0):\n",
    "        fn = fn + 1\n",
    "    elif (r[\"true\"] == 0) and (r[\"pred\"] == 1):\n",
    "        fp = fp + 1\n",
    "    elif (r[\"true\"] == 0) and (r[\"pred\"] == 0):\n",
    "        tn = tn + 1\n",
    "\n",
    "print(\"tp: \" + str(tp))\n",
    "print(\"tn: \" + str(tn))\n",
    "print(\"fp: \" + str(fp))\n",
    "print(\"fn: \" + str(fn))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
